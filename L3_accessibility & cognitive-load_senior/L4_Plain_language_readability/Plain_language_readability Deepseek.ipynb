{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0fef59d-4191-4124-b5bd-605fe6014aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek V3.2 works!\n",
      "Hello!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"xxx\",\n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        max_tokens=50,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Say hi in one short sentence.\"}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"DeepSeek V3.2 works!\")\n",
    "    print(resp.choices[0].message.content)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"DeepSeek API key error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b978d400-9d7c-4c23-b246-9ff4259e042c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1/20 | Letter=L\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 2/20 | Letter=L\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 3/20 | Letter=L\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 4/20 | Letter=L\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 5/20 | Letter=L\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 6/20 | Letter=S\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 7/20 | Letter=S\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 8/20 | Letter=S\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 9/20 | Letter=S\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 10/20 | Letter=S\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 11/20 | Letter=C\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 12/20 | Letter=C\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 13/20 | Letter=C\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 14/20 | Letter=C\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 15/20 | Letter=C\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 16/20 | Letter=R\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 17/20 | Letter=R\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 18/20 | Letter=R\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 19/20 | Letter=R\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 20/20 | Letter=R\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "\n",
      "JSON written to: L4 3_deepseek.json\n",
      "\n",
      "=== Sub-indicator Mean Scores ===\n",
      "L (Lexical Simplicity):                1.040   (52.0%)\n",
      "S (Sentence Brevity & Structure):      1.000   (50.0%)\n",
      "C (Conceptual Clarity & Cog. Load):    1.000   (50.0%)\n",
      "R (Readability Level Compliance):      1.080   (54.0%)\n",
      "\n",
      "Overall L4 Score (LSCR): 1.028   (51.4%)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "L4 Plain-language & Readability Thresholds (senior-appropriate)\n",
    "End-to-end evaluation script using Claude API.\n",
    "\n",
    "Inputs:\n",
    "    - CSV file with columns:\n",
    "        Category(L4), Source, Sub-Indicators, Letter, Prompt\n",
    "\n",
    "Outputs:\n",
    "    - JSON file with model answers and scores per prompt\n",
    "    - Printed summary of mean scores per sub-indicator and overall L4 score\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "MODEL_NAME = \"deepseek-chat\"\n",
    "\n",
    "CSV_PATH = \"Plain_language_readability.csv\"\n",
    "OUTPUT_JSON_PATH = \"Plain_language_readability_deepseek.json\"\n",
    "\n",
    "# Number of repetitions of the prompt\n",
    "N_RUNS = 5\n",
    "\n",
    "\n",
    "# Rubrics\n",
    "LEXICAL_RUBRIC = \"\"\"\n",
    "Dimension 1 — L: Lexical Simplicity (Weight 0.30)\n",
    "\n",
    "Measures whether vocabulary is simple, familiar, and free of jargon.\n",
    "\n",
    "0 – Not simple\n",
    "- Uses difficult or technical terms without explanation.\n",
    "- Includes jargon (e.g., “authentication,” “optimize,” “synchronize”).\n",
    "- Vocabulary level clearly above senior-friendly range.\n",
    "\n",
    "1 – Partially simple\n",
    "- Mostly simple words but still includes complex terms.\n",
    "- Inconsistent explanation of unfamiliar words.\n",
    "- Some jargon remains.\n",
    "\n",
    "2 – Fully simple\n",
    "- All vocabulary is clear, common, and everyday.\n",
    "- Any necessary technical term is immediately explained.\n",
    "- No jargon, no abstract or academic vocabulary.\n",
    "\"\"\"\n",
    "\n",
    "SENTENCE_RUBRIC = \"\"\"\n",
    "Dimension 2 — S: Sentence Brevity & Structure (Weight 0.30)\n",
    "\n",
    "Measures whether sentences are short, direct, and structurally simple.\n",
    "\n",
    "0 – Too complex / long\n",
    "- Long sentences.\n",
    "- Multiple clauses or nested structures.\n",
    "- Hard for seniors to track.\n",
    "\n",
    "1 – Moderately simple\n",
    "- Some short sentences, but also a few long or compound ones.\n",
    "- Occasional structural complexity.\n",
    "\n",
    "2 – Very simple\n",
    "- Mostly short sentences.\n",
    "- Each sentence expresses only one action or idea.\n",
    "- No unnecessary clauses.\n",
    "\"\"\"\n",
    "\n",
    "CONCEPT_RUBRIC = \"\"\"\n",
    "Dimension 3 — C: Conceptual Clarity & Cognitive Load (Weight 0.20)\n",
    "\n",
    "Measures whether the explanation minimizes mental effort and avoids overloading seniors.\n",
    "\n",
    "0 – High cognitive load\n",
    "- Gives too many ideas at once.\n",
    "- Explanations are abstract or overly conceptual.\n",
    "- No chunking or stepwise support.\n",
    "\n",
    "1 – Moderate\n",
    "- Clear overall, but some steps combine multiple ideas.\n",
    "- Occasional conceptual density.\n",
    "\n",
    "2 – Very clear, low load\n",
    "- One idea at a time.\n",
    "- Clear chunking of information.\n",
    "- No abstract concepts, no overload.\n",
    "- Easy for seniors with low working memory.\n",
    "\"\"\"\n",
    "\n",
    "READABILITY_RUBRIC = \"\"\"\n",
    "Dimension 4 — R: Readability Level (Weight 0.20)\n",
    "\n",
    "Measures whether the text naturally meets senior readability thresholds (approx. 6th–8th grade).\n",
    "\n",
    "0 – High reading level (10th+ grade)\n",
    "- Complex vocabulary and sentence structure.\n",
    "- Clearly difficult for seniors.\n",
    "\n",
    "1 – Medium reading level (8th–10th grade)\n",
    "- Mostly simple, but some elements still too advanced.\n",
    "\n",
    "2 – Meets senior-friendly readability (6th–8th grade)\n",
    "- Very easy to read.\n",
    "- Matches plain-language standards recommended for aging adults.\n",
    "\"\"\"\n",
    "\n",
    "# DeepSeek helper functions\n",
    "def call_deepseek(\n",
    "    user_prompt: str,\n",
    "    system_prompt: str | None = None,\n",
    "    max_tokens: int = 1024,\n",
    "    temperature: float = 0.0,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    General-purpose DeepSeek call wrapper.\n",
    "    Returns plain text output.\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def generate_answer(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate the model's natural-language answer for readability evaluation.\n",
    "    The system prompt is intentionally light so the model's natural style\n",
    "    is preserved without artificially improving readability.\n",
    "    \"\"\"\n",
    "    system_msg = (\n",
    "        \"You are a helpful and polite AI assistant. Provide a clear and complete \"\n",
    "        \"answer to the user's question, in your natural writing style. \"\n",
    "        \"Do not simplify your vocabulary or shorten your sentences intentionally.\"\n",
    "    )\n",
    "\n",
    "    return call_deepseek(\n",
    "        prompt,\n",
    "        system_prompt=system_msg,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_json_object(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    DeepSeek may output additional text around JSON.\n",
    "    First try json.loads; if it fails, extract the first {...} block.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Could not find JSON in DeepSeek response: {text[:200]}\")\n",
    "    return json.loads(match.group(0))\n",
    "\n",
    "\n",
    "\n",
    "def score_single_dimension(letter: str, prompt: str, answer: str) -> int:\n",
    "    \"\"\"\n",
    "    Strict scoring for L/S/C/R based ONLY on the chosen dimension.\n",
    "    Uses your rubric plus strict operational rules.\n",
    "    \"\"\"\n",
    "\n",
    "    if letter == \"L\":\n",
    "        dimension = \"Lexical Simplicity\"\n",
    "        rubric = LEXICAL_RUBRIC\n",
    "        extra_rule = \"\"\"\n",
    "Scoring rule for Lexical Simplicity (STRICT):\n",
    "\n",
    "- Start from 1 if the answer is generally understandable.\n",
    "- Give **2** ONLY IF ALL conditions hold:\n",
    "  • Vocabulary is entirely common, everyday, and senior-friendly.\n",
    "  • Any necessary technical term is immediately explained in plain words.\n",
    "  • No remaining jargon, abstract academic words, or unexplained acronyms.\n",
    "- If there are complex terms, mixed with simpler wording, or inconsistent explanations → **1**.\n",
    "- If vocabulary is clearly too technical, jargon-heavy, or hard for seniors → **0**.\n",
    "\"\"\"\n",
    "    elif letter == \"S\":\n",
    "        dimension = \"Sentence Brevity & Structure\"\n",
    "        rubric = SENTENCE_RUBRIC\n",
    "        extra_rule = \"\"\"\n",
    "Scoring rule for Sentence Brevity & Structure (BALANCED STRICT):\n",
    "\n",
    "- Default starting point: **1** if the answer is generally followable.\n",
    "\n",
    "Give **2** ONLY IF ALL conditions hold:\n",
    "  • Most sentences are short.\n",
    "  • Each sentence expresses only one clear idea or action.\n",
    "  • Very few (or no) unnecessary clauses or nested structures.\n",
    "  • The structure is consistently clean and easy for seniors to track.\n",
    "\n",
    "Give **1** if:\n",
    "  • There is a mix of short and somewhat long sentences, OR\n",
    "  • Some sentences contain clauses but remain understandable, OR\n",
    "  • The structure is mostly clear even if not perfectly simple.\n",
    "\n",
    "Give **0** ONLY IF:\n",
    "  • Many sentences are very long, OR\n",
    "  • Multiple nested clauses significantly increase complexity, OR\n",
    "  • Ideas are merged together and structure becomes hard to follow for seniors.\n",
    "\"\"\"\n",
    "    elif letter == \"C\":\n",
    "        dimension = \"Conceptual Clarity & Cognitive Load\"\n",
    "        rubric = CONCEPT_RUBRIC\n",
    "        extra_rule = \"\"\"\n",
    "Scoring rule for Conceptual Clarity & Cognitive Load (STRICT):\n",
    "\n",
    "- Start from 1 if the explanation is mostly understandable.\n",
    "- Give **2** ONLY IF:\n",
    "  • The answer introduces one idea at a time.\n",
    "  • Information is clearly chunked (e.g., separated steps or short paragraphs).\n",
    "  • There are no abstract or overly conceptual explanations.\n",
    "  • Overall mental effort required for seniors is low.\n",
    "- If some steps combine multiple ideas or there is occasional conceptual density → **1**.\n",
    "- If many ideas are given at once, explanations are abstract, or there is no chunking → **0**.\n",
    "\"\"\"\n",
    "    elif letter == \"R\":\n",
    "        dimension = \"Readability Level Compliance\"\n",
    "        rubric = READABILITY_RUBRIC\n",
    "        extra_rule = \"\"\"\n",
    "Scoring rule for Readability Level (STRICT):\n",
    "\n",
    "- Base your judgment on overall sentence length, vocabulary difficulty,\n",
    "  and how it would score on typical readability indices (e.g., FKGL/SMOG),\n",
    "  even if you estimate informally.\n",
    "\n",
    "- Give **2** ONLY IF:\n",
    "  • The text clearly feels like 6th–8th grade reading level.\n",
    "  • It is very easy to read for aging adults with average literacy.\n",
    "\n",
    "- If it feels around 8th–10th grade (mostly simple but with some advanced elements) → **1**.\n",
    "- If it clearly requires high-school level (10th+ grade) or above → **0**.\n",
    "\"\"\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown letter: {letter}\")\n",
    "\n",
    "    scoring_prompt = f\"\"\"\n",
    "You are a STRICT evaluator for the dimension: **{dimension}**.\n",
    "Ignore helpfulness, friendliness, or factual correctness. \n",
    "Rate ONLY the plain-language and readability quality according to the rubric.\n",
    "\n",
    "IMPORTANT:\n",
    "- You MUST be conservative.\n",
    "- Score **2 ONLY IF ALL requirements for the 2-point level are fully satisfied**.\n",
    "- If ANY part does NOT match the 2-point definition → score at most 1.\n",
    "- If the answer resembles 0-point issues → score 0.\n",
    "\n",
    "User prompt:\n",
    "\\\"\\\"\\\"{prompt}\\\"\\\"\\\"\n",
    "\n",
    "Model answer:\n",
    "\\\"\\\"\\\"{answer}\\\"\\\"\\\"\n",
    "\n",
    "Rubric:\n",
    "{rubric}\n",
    "\n",
    "Additional strict scoring rules:\n",
    "{extra_rule}\n",
    "\n",
    "Return JSON ONLY with this exact schema:\n",
    "{{\n",
    "  \"score\": 0\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    raw = call_deepseek(scoring_prompt, max_tokens=256, temperature=0.0)\n",
    "    data = extract_json_object(raw)\n",
    "\n",
    "    score = int(data[\"score\"])\n",
    "    if score < 0:\n",
    "        score = 0\n",
    "    if score > 2:\n",
    "        score = 2\n",
    "    return score\n",
    "\n",
    "# Run evaluation over CSV\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df = df.dropna(subset=[\"Prompt\"]).reset_index(drop=True)\n",
    "results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    category = row[\"Category(L4)\"]\n",
    "    sub_indicator = row[\"Sub-Indicators\"]\n",
    "    letter = row[\"Letter\"]\n",
    "    prompt = row[\"Prompt\"]\n",
    "\n",
    "    print(f\"Processing {idx+1}/{len(df)} | Letter={letter}\")\n",
    "\n",
    "    run_scores = []\n",
    "    run_answers = []\n",
    "\n",
    "    for run in range(N_RUNS):\n",
    "        print(f\"  Run {run+1}/{N_RUNS}\")\n",
    "        answer = generate_answer(prompt)\n",
    "        score = score_single_dimension(letter, prompt, answer)\n",
    "\n",
    "        run_scores.append(score)\n",
    "        run_answers.append(f\"{run+1}. {answer}\")\n",
    "\n",
    "    avg_score = sum(run_scores) / len(run_scores)\n",
    "\n",
    "\n",
    "    results.append({\n",
    "        \"category\": category,\n",
    "        \"sub_indicator\": sub_indicator,\n",
    "        \"letter\": letter,\n",
    "        \"prompt\": prompt,\n",
    "        \"answers\": run_answers,\n",
    "        \"scores_all_runs\": run_scores,\n",
    "        \"score\": avg_score, \n",
    "    })\n",
    "\n",
    "with open(OUTPUT_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nJSON written to: {OUTPUT_JSON_PATH}\")\n",
    "\n",
    "# Aggregate & print summary\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "L_mean = results_df[results_df[\"letter\"] == \"L\"][\"score\"].mean()\n",
    "S_mean = results_df[results_df[\"letter\"] == \"S\"][\"score\"].mean()\n",
    "C_mean = results_df[results_df[\"letter\"] == \"C\"][\"score\"].mean()\n",
    "R_mean = results_df[results_df[\"letter\"] == \"R\"][\"score\"].mean()\n",
    "\n",
    "L4_score = (\n",
    "    0.30 * L_mean +\n",
    "    0.30 * S_mean +\n",
    "    0.20 * C_mean +\n",
    "    0.20 * R_mean\n",
    ")\n",
    "\n",
    "# Percentages (2 means 100%)\n",
    "L_pct = L_mean / 2 * 100\n",
    "S_pct = S_mean / 2 * 100\n",
    "C_pct = C_mean / 2 * 100\n",
    "R_pct = R_mean / 2 * 100\n",
    "L4_pct = L4_score / 2 * 100\n",
    "\n",
    "print(\"\\n=== Sub-indicator Mean Scores ===\")\n",
    "print(f\"L (Lexical Simplicity):                {L_mean:.3f}   ({L_pct:.1f}%)\")\n",
    "print(f\"S (Sentence Brevity & Structure):      {S_mean:.3f}   ({S_pct:.1f}%)\")\n",
    "print(f\"C (Conceptual Clarity & Cog. Load):    {C_mean:.3f}   ({C_pct:.1f}%)\")\n",
    "print(f\"R (Readability Level Compliance):      {R_mean:.3f}   ({R_pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nOverall L4 Score (LSCR): {L4_score:.3f}   ({L4_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2f5544-dd67-4b9e-b583-af3400349618",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ds701python]",
   "language": "python",
   "name": "conda-env-ds701python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
