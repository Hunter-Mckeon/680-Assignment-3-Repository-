{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47d378c5-09b3-48d2-a72d-ca14bf185d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek V3.2 works!\n",
      "Hello there!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"xxx\",\n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        max_tokens=50,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Say hi in one short sentence.\"}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"DeepSeek V3.2 works!\")\n",
    "    print(resp.choices[0].message.content)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"DeepSeek API key error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1373bb38-60a0-424e-a96c-8b6e84896528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1/20 | Letter=T\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 2/20 | Letter=T\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 3/20 | Letter=T\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 4/20 | Letter=T\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 5/20 | Letter=T\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 6/20 | Letter=E\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 7/20 | Letter=E\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 8/20 | Letter=E\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 9/20 | Letter=E\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 10/20 | Letter=E\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 11/20 | Letter=P\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 12/20 | Letter=P\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 13/20 | Letter=P\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 14/20 | Letter=P\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 15/20 | Letter=P\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 16/20 | Letter=I\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 17/20 | Letter=I\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 18/20 | Letter=I\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 19/20 | Letter=I\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "Processing 20/20 | Letter=I\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "\n",
      "JSON written to: L4 user education claude.json\n",
      "\n",
      "=== Sub-indicator Mean Scores ===\n",
      "T (Tutorial Structure):             2.000   (100.0%)\n",
      "E (Explanation Quality):            2.000   (100.0%)\n",
      "P (Progressive Skill-Building):     2.000   (100.0%)\n",
      "I (Interactive Clarification):      1.160   (58.0%)\n",
      "\n",
      "Overall L4 Score (User Education Scaffolds): 1.832   (91.6%)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "L4 User Education Scaffolds (tutorials, explainers)\n",
    "End-to-end evaluation script using Claude API.\n",
    "\n",
    "Inputs:\n",
    "    - CSV file with columns:\n",
    "        Category(L4), Source, Sub-Indicators, Letter, Prompt\n",
    "\n",
    "Outputs:\n",
    "    - JSON file with model answers and scores per prompt\n",
    "    - Printed summary of mean scores per sub-indicator and overall L4 score\n",
    "\n",
    "Sub-indicators & weights:\n",
    "    T = Tutorial Structure             (0.30)\n",
    "    E = Explanation Quality            (0.30)\n",
    "    P = Progressive Skill-Building     (0.20)\n",
    "    I = Interactive Clarification      (0.20)\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "MODEL_NAME = \"deepseek-chat\"\n",
    "\n",
    "CSV_PATH = \"L4 user education.csv\"\n",
    "OUTPUT_JSON_PATH = \"L4 user education deepseek.json\"\n",
    "\n",
    "# Number of repetitions of the prompt\n",
    "N_RUNS = 5\n",
    "\n",
    "# Rubrics\n",
    "TUTORIAL_RUBRIC = \"\"\"\n",
    "1. T — Tutorial Structure (0–2)\n",
    "What it measures: Whether the model organizes instructions into a clear, tutorial-like structure.\n",
    "\n",
    "0 – No structure\n",
    "• Response is a long, unorganized paragraph.\n",
    "• No steps, sections, or headings.\n",
    "• User cannot identify a clear sequence.\n",
    "\n",
    "1 – Partial structure\n",
    "• Some structure appears but is incomplete or inconsistent.\n",
    "• Steps are merged, unclear, or missing.\n",
    "• Parts of the explanation remain disorganized.\n",
    "\n",
    "2 – Full structure\n",
    "• Clear step-by-step or section-based organization.\n",
    "• Steps are labeled and logically ordered.\n",
    "• Each step contains a single actionable idea.\n",
    "\"\"\"\n",
    "\n",
    "EXPLANATION_RUBRIC = \"\"\"\n",
    "2. E — Explanation Quality (0–2)\n",
    "What it measures: Whether the explanation is accurate, simple, and accessible.\n",
    "\n",
    "0 – Poor explanation\n",
    "• Vague, overly technical, or incorrect.\n",
    "• Uses jargon without explanation.\n",
    "• Difficult for beginners to understand.\n",
    "\n",
    "1 – Adequate explanation\n",
    "• Mostly correct and understandable.\n",
    "• Occasional jargon or unclear phrasing.\n",
    "• Not fully optimized for novices.\n",
    "\n",
    "2 – High-quality explanation\n",
    "• Clear, accurate, beginner-friendly wording.\n",
    "• Uses plain language and helpful examples.\n",
    "• Avoids unnecessary complexity.\n",
    "\"\"\"\n",
    "\n",
    "PROGRESSION_RUBRIC = \"\"\"\n",
    "3. P — Progressive Skill-Building (0–2)\n",
    "What it measures: Whether the model gradually increases complexity.\n",
    "\n",
    "0 – No progression\n",
    "• Provides all information at once.\n",
    "• No simple-to-complex staging.\n",
    "• Likely to overwhelm beginners.\n",
    "\n",
    "1 – Partial progression\n",
    "• Some progression appears but inconsistently.\n",
    "• Transitions exist but are not well controlled.\n",
    "• Some sections contain excessive information.\n",
    "\n",
    "2 – Strong progression\n",
    "• Begins with basics, adds complexity gradually.\n",
    "• Builds skills step-by-step.\n",
    "• Reduces cognitive load effectively.\n",
    "\"\"\"\n",
    "\n",
    "INTERACTIVE_RUBRIC = \"\"\"\n",
    "4. I — Interactive Clarification (0–2)\n",
    "What it measures: Whether the model checks understanding or adapts interactively.\n",
    "\n",
    "0 – No interaction\n",
    "• No clarifying questions.\n",
    "• No checks for understanding.\n",
    "• One-directional explanation.\n",
    "\n",
    "1 – Limited interaction\n",
    "• Occasional clarification or check-in.\n",
    "• Some responsiveness but inconsistent.\n",
    "• Interaction does not fully guide learning.\n",
    "\n",
    "2 – Full interactive scaffolding\n",
    "• Actively checks user understanding.\n",
    "• Offers choices or alternate paths.\n",
    "• Adjusts explanations like a tutor.\n",
    "\"\"\"\n",
    "\n",
    "# DeepSeek helper functions\n",
    "def call_deepseek(\n",
    "    user_prompt: str,\n",
    "    system_prompt: str | None = None,\n",
    "    max_tokens: int = 1024,\n",
    "    temperature: float = 0.0,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    General-purpose DeepSeek call wrapper.\n",
    "    Returns plain text output.\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "def generate_answer(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate the model's natural explanation for the user-education task.\n",
    "    The prompt is intentionally light so the model is not biased toward any\n",
    "    specific scoring dimension (T/E/P/I).\n",
    "    \"\"\"\n",
    "    system_msg = (\n",
    "        \"You are a helpful AI assistant. Provide a clear and useful response \"\n",
    "        \"to help the user learn or understand the topic.\"\n",
    "    )\n",
    "    return call_deepseek(prompt, system_prompt=system_msg,\n",
    "                         max_tokens=1024, temperature=0.5)\n",
    "\n",
    "def extract_json_object(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    DeepSeek may output additional text around JSON.\n",
    "    First try json.loads; if it fails, extract the first {...} block.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Could not find JSON in DeepSeek response: {text[:200]}\")\n",
    "    return json.loads(match.group(0))\n",
    "\n",
    "\n",
    "# Scoring per sub-indicator\n",
    "def score_single_dimension(letter: str, prompt: str, answer: str) -> int:\n",
    "    \"\"\"\n",
    "    Scoring for T/E/P/I based ONLY on the chosen dimension.\n",
    "    Uses the rubric plus balanced (slightly generous) operational rules,\n",
    "    to avoid ultra-strict under-scoring.\n",
    "    \"\"\"\n",
    "\n",
    "    if letter == \"T\":\n",
    "        dimension = \"Tutorial Structure\"\n",
    "        rubric = TUTORIAL_RUBRIC\n",
    "        extra_rule = \"\"\"\n",
    "Scoring guide (balanced, slightly generous):\n",
    "\n",
    "• Start from score 1 if there is ANY visible structure (lists, steps, headings).\n",
    "• Give **2** when the structure is clearly tutorial-like:\n",
    "  - Most steps are labeled or clearly separated.\n",
    "  - Order roughly follows a logical sequence.\n",
    "  - Many steps are actionable, even if a few contain 2 small actions.\n",
    "• Give **1** when structure exists but is messy, incomplete, or partly merged.\n",
    "• Give **0** only when it is basically one big block with no clear sequence.\n",
    "\"\"\"\n",
    "    elif letter == \"E\":\n",
    "        dimension = \"Explanation Quality\"\n",
    "        rubric = EXPLANATION_RUBRIC\n",
    "        extra_rule = \"\"\"\n",
    "Scoring guide (balanced, slightly generous):\n",
    "\n",
    "• Start from score 1 if the explanation is mostly understandable.\n",
    "• Give **2** when:\n",
    "  - The explanation is generally accurate,\n",
    "  - Uses mostly plain, beginner-friendly language,\n",
    "  - Provides at least one simple example or analogy OR clearly avoids heavy jargon.\n",
    "  Small bits of mild jargon or one slightly long sentence are acceptable.\n",
    "• Give **1** when it is understandable but has noticeable jargon, complexity,\n",
    "  or missing examples that would help a beginner.\n",
    "• Give **0** only if it is confusing, very technical, or likely to mislead beginners.\n",
    "\"\"\"\n",
    "    elif letter == \"P\":\n",
    "        dimension = \"Progressive Skill-Building\"\n",
    "        rubric = PROGRESSION_RUBRIC\n",
    "        extra_rule = \"\"\"\n",
    "Scoring guide (balanced, slightly generous):\n",
    "\n",
    "• Start from score 1 if there is at least some sense of \"start here, then go deeper\".\n",
    "• Give **2** when:\n",
    "  - The answer clearly starts with basics,\n",
    "  - Then introduces more advanced ideas or extra steps,\n",
    "  - And the stages feel manageable for a novice (not all dumped at once).\n",
    "  It does not need to be perfect; minor information overload is acceptable.\n",
    "• Give **1** when there is some progression, but stages are uneven, or a lot of detail\n",
    "  appears too early.\n",
    "• Give **0** when almost everything is delivered at once with no visible staging.\n",
    "\"\"\"\n",
    "    elif letter == \"I\":\n",
    "        dimension = \"Interactive Clarification\"\n",
    "        rubric = INTERACTIVE_RUBRIC\n",
    "        extra_rule = \"\"\"\n",
    "Scoring guide (balanced, slightly generous):\n",
    "\n",
    "• Start from score 0 and look specifically for INTERACTION.\n",
    "• Give **2** when:\n",
    "  - The answer clearly asks one or more questions to the user,\n",
    "  - OR offers explicit choices / branches (e.g., “If you are using a phone, do X; if a computer, do Y”),\n",
    "  - AND uses these questions/branches to adapt the explanation or next steps.\n",
    "  It does not need to be a full conversation, but it should feel like tutoring, not a monologue.\n",
    "• Give **1** when there is some interaction (one or two check-in questions,\n",
    "  invitations like “let me know if…”, or mild branching) but it is limited or not\n",
    "  used strongly to adapt the explanation.\n",
    "• Give **0** when the answer is purely one-way explanation with no questions,\n",
    "  no checks for understanding, and no real choice offered to the user.\n",
    "\"\"\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown letter: {letter}\")\n",
    "\n",
    "    scoring_prompt = f\"\"\"\n",
    "You are evaluating ONLY the dimension: **{dimension}**.\n",
    "\n",
    "Task:\n",
    "- Ignore overall helpfulness, friendliness, or correctness outside this dimension.\n",
    "- Use the rubric and scoring guide below.\n",
    "- Be fair and slightly generous: if an answer clearly matches MOST of the 2-point\n",
    "  description and only has minor issues, give **2** instead of **1**.\n",
    "\n",
    "User prompt:\n",
    "\\\"\\\"\\\"{prompt}\\\"\\\"\\\"\n",
    "\n",
    "Model answer:\n",
    "\\\"\\\"\\\"{answer}\\\"\\\"\\\"\n",
    "\n",
    "Rubric:\n",
    "{rubric}\n",
    "\n",
    "Operational scoring guide:\n",
    "{extra_rule}\n",
    "\n",
    "Return JSON ONLY with this schema:\n",
    "{{\n",
    "  \"score\": 0\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    raw = call_deepseek(scoring_prompt, max_tokens=256, temperature=0.0)\n",
    "    data = extract_json_object(raw)\n",
    "\n",
    "    score = int(data[\"score\"])\n",
    "    if score < 0:\n",
    "        score = 0\n",
    "    if score > 2:\n",
    "        score = 2\n",
    "    return score\n",
    "\n",
    "\n",
    "# Main evaluation loop\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df = df.dropna(subset=[\"Prompt\"]).reset_index(drop=True)\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    category = row[\"Category(L4)\"]\n",
    "    sub_indicator = row[\"Sub-Indicators\"]\n",
    "    letter = row[\"Letter\"]\n",
    "    prompt = row[\"Prompt\"]\n",
    "\n",
    "    print(f\"Processing {idx+1}/{len(df)} | Letter={letter}\")\n",
    "\n",
    "    run_scores = []\n",
    "    run_answers = []\n",
    "\n",
    "    for run in range(N_RUNS):\n",
    "        print(f\"  Run {run+1}/{N_RUNS}\")\n",
    "        answer = generate_answer(prompt)\n",
    "        score = score_single_dimension(letter, prompt, answer)\n",
    "\n",
    "        run_scores.append(score)\n",
    "        run_answers.append(f\"{run+1}. {answer}\")\n",
    "\n",
    "    avg_score = sum(run_scores) / len(run_scores)\n",
    "\n",
    "\n",
    "    results.append({\n",
    "        \"category\": category,\n",
    "        \"sub_indicator\": sub_indicator,\n",
    "        \"letter\": letter,\n",
    "        \"prompt\": prompt,\n",
    "        \"answers\": run_answers,\n",
    "        \"scores_all_runs\": run_scores,\n",
    "        \"score\": avg_score, \n",
    "    })\n",
    "\n",
    "with open(OUTPUT_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nJSON written to: {OUTPUT_JSON_PATH}\")\n",
    "\n",
    "# Aggregation & print summary\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "T_mean = results_df[results_df[\"letter\"] == \"T\"][\"score\"].mean()\n",
    "E_mean = results_df[results_df[\"letter\"] == \"E\"][\"score\"].mean()\n",
    "P_mean = results_df[results_df[\"letter\"] == \"P\"][\"score\"].mean()\n",
    "I_mean = results_df[results_df[\"letter\"] == \"I\"][\"score\"].mean()\n",
    "\n",
    "L4_score = (\n",
    "    0.30 * T_mean +\n",
    "    0.30 * E_mean +\n",
    "    0.20 * P_mean +\n",
    "    0.20 * I_mean\n",
    ")\n",
    "\n",
    "# Percentages (2 means 100%)\n",
    "T_pct = T_mean / 2 * 100\n",
    "E_pct = E_mean / 2 * 100\n",
    "P_pct = P_mean / 2 * 100\n",
    "I_pct = I_mean / 2 * 100\n",
    "L4_pct = L4_score / 2 * 100\n",
    "\n",
    "print(\"\\n=== Sub-indicator Mean Scores ===\")\n",
    "print(f\"T (Tutorial Structure):             {T_mean:.3f}   ({T_pct:.1f}%)\")\n",
    "print(f\"E (Explanation Quality):            {E_mean:.3f}   ({E_pct:.1f}%)\")\n",
    "print(f\"P (Progressive Skill-Building):     {P_mean:.3f}   ({P_pct:.1f}%)\")\n",
    "print(f\"I (Interactive Clarification):      {I_mean:.3f}   ({I_pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nOverall L4 Score (User Education Scaffolds): {L4_score:.3f}   ({L4_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f30abb7-be3a-4c1a-960a-76ea7993c40c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ds701python]",
   "language": "python",
   "name": "conda-env-ds701python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
